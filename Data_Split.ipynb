{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "XfGrai_Qt7Ny"
      },
      "source": [
        "# import all libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import random_split\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torch.backends.cudnn as cudnn\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import os\n",
        "import argparse\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VgAiImV0uURP"
      },
      "source": [
        "# these are commonly used data augmentations\n",
        "# random cropping and random horizontal flip\n",
        "# lastly, we normalize each channel into zero mean and unit standard deviation\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(\n",
        "    root='./data', train=True, download=True, transform=transform_train)\n",
        "trainloader = torch.utils.data.DataLoader(\n",
        "    trainset, batch_size=128, shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(\n",
        "    root='./data', train=False, download=True, transform=transform_test)\n",
        "\n",
        "# we can use a larger batch size during test, because we do not save\n",
        "# intermediate variables for gradient computation, which leaves more memory\n",
        "testloader = torch.utils.data.DataLoader(\n",
        "    testset, batch_size=256, shuffle=False, num_workers=2)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer',\n",
        "           'dog', 'frog', 'horse', 'ship', 'truck')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the size of the dataset\n",
        "print(f\"Training set size: {len(trainset)}\")\n",
        "print(f\"Test set size: {len(testset)}\")  # if you have testset defined"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xX3SqP4sKgzO",
        "outputId": "42d9caea-6f14-40aa-a2d9-e4ec4b44df55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set size: 50000\n",
            "Test set size: 10000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o9e4QhB5B89H"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train-Val split"
      ],
      "metadata": {
        "id": "0cWCVj6qLL0J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split into train (40k) and validation (10k)\n",
        "train_size = 40000\n",
        "val_size = 10000\n",
        "trainset, valset = random_split(trainset, [train_size, val_size])\n",
        "\n",
        "\n",
        "# Check sizes\n",
        "print(f\"Training set: {len(trainset)}\")      # 40,000\n",
        "print(f\"Validation set: {len(valset)}\")      # 10,000\n",
        "print(f\"Test set: {len(testset)}\")           # 10,000 - unchanged\n",
        "\n",
        "# Create DataLoaders\n",
        "trainloader = torch.utils.data.DataLoader(\n",
        "    trainset, batch_size=128, shuffle=True, num_workers=2)\n",
        "valloader = torch.utils.data.DataLoader(\n",
        "    valset, batch_size=128, shuffle=False, num_workers=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bhHbPJOCLPwE",
        "outputId": "e4ac67bd-8260-4002-fc14-a50946b9f11b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set: 40000\n",
            "Validation set: 10000\n",
            "Test set: 10000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "# Save the split datasets\n",
        "print(\"Saving train/val/test datasets...\")\n",
        "\n",
        "# Save train dataset (the Subset object)\n",
        "with open('./data/train_dataset.pkl', 'wb') as f:\n",
        "    pickle.dump(trainset, f)\n",
        "print(f\"✅ Saved train_dataset.pkl: {len(trainset)} samples\")\n",
        "\n",
        "# Save val dataset (the Subset object)\n",
        "with open('./data/val_dataset.pkl', 'wb') as f:\n",
        "    pickle.dump(valset, f)\n",
        "print(f\"✅ Saved val_dataset.pkl: {len(valset)} samples\")\n",
        "\n",
        "# Save test dataset\n",
        "with open('./data/test_dataset.pkl', 'wb') as f:\n",
        "    pickle.dump(testset, f)\n",
        "print(f\"✅ Saved test_dataset.pkl: {len(testset)} samples\")\n",
        "\n",
        "print(\"\\nNow downloading files...\")\n",
        "\n",
        "# Download the files\n",
        "from google.colab import files\n",
        "files.download('./data/train_dataset.pkl')\n",
        "files.download('./data/val_dataset.pkl')\n",
        "files.download('./data/test_dataset.pkl')"
      ],
      "metadata": {
        "id": "RnOuf7764I1k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#reason - same train-val-test split needed for each experiment. each experiment is ran separately via GPU cluster so that the best params for each section can be chosen"
      ],
      "metadata": {
        "id": "QsXjidYb4W2t"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}