{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "XfGrai_Qt7Ny"
      },
      "source": [
        "# import all libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import random_split\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torch.backends.cudnn as cudnn\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import os\n",
        "import argparse\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VgAiImV0uURP"
      },
      "source": [
        "import pickle\n",
        "\n",
        "# Define transforms (keep these - they're still needed!)\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "# Load the saved datasets from pickle files\n",
        "print(\"Loading datasets from pickle files...\")\n",
        "\n",
        "with open('./data/train_dataset.pkl', 'rb') as f:\n",
        "    trainset = pickle.load(f)\n",
        "\n",
        "with open('./data/val_dataset.pkl', 'rb') as f:\n",
        "    valset = pickle.load(f)\n",
        "\n",
        "# Don't load test for Section 4 (not needed)\n",
        "# with open('./data/test_dataset.pkl', 'rb') as f:\n",
        "#     testset = pickle.load(f)\n",
        "\n",
        "print(f\"âœ… Loaded! Train={len(trainset)}, Val={len(valset)}\")\n",
        "\n",
        "# Create DataLoaders\n",
        "trainloader = torch.utils.data.DataLoader(\n",
        "    trainset, batch_size=128, shuffle=True, num_workers=2)\n",
        "valloader = torch.utils.data.DataLoader(\n",
        "    valset, batch_size=128, shuffle=False, num_workers=2)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer',\n",
        "           'dog', 'frog', 'horse', 'ship', 'truck')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o9e4QhB5B89H"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hldipDVsv-Jt"
      },
      "source": [
        "# Training\n",
        "def train(epoch, net, criterion, trainloader, scheduler=None):\n",
        "    device = 'cuda'\n",
        "    print('\\nEpoch: %d' % epoch)\n",
        "    net.train()\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "        if (batch_idx+1) % 50 == 0:\n",
        "          print(\"iteration : %3d, loss : %0.4f, accuracy : %2.2f\" % (batch_idx+1, train_loss/(batch_idx+1), 100.*correct/total))\n",
        "\n",
        "    if scheduler is not None:\n",
        "        scheduler.step()\n",
        "    return train_loss/(batch_idx+1), 100.*correct/total"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vgyCI0U08i2h"
      },
      "source": [
        "Test performance on the test set. Note the use of `torch.inference_mode()`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VkooK-hQu4a6"
      },
      "source": [
        "def test(epoch, net, criterion, testloader):\n",
        "    device = 'cuda'\n",
        "    net.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.inference_mode():\n",
        "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            test_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "    return test_loss/(batch_idx+1), 100.*correct/total\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jEj8J7xqwAxD"
      },
      "source": [
        "def save_checkpoint(net, acc, epoch):\n",
        "    # Save checkpoint.\n",
        "    print('Saving..')\n",
        "    state = {\n",
        "        'net': net.state_dict(),\n",
        "        'acc': acc,\n",
        "        'epoch': epoch,\n",
        "    }\n",
        "    if not os.path.isdir('checkpoint'):\n",
        "        os.mkdir('checkpoint')\n",
        "    torch.save(state, './checkpoint/ckpt.pth')\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomBatchNorm2d(nn.Module):\n",
        "    \"\"\"\n",
        "    Custom Batch Normalization where mean and std do NOT participate\n",
        "    in gradient calculation using detach().\n",
        "    \"\"\"\n",
        "    def __init__(self, num_features, eps=1e-5, momentum=0.1):\n",
        "        super(CustomBatchNorm2d, self).__init__()\n",
        "        self.num_features = num_features\n",
        "        self.eps = eps\n",
        "        self.momentum = momentum\n",
        "\n",
        "        # Learnable parameters (gamma and beta)\n",
        "        self.weight = nn.Parameter(torch.ones(num_features))\n",
        "        self.bias = nn.Parameter(torch.zeros(num_features))\n",
        "\n",
        "        # Running statistics for inference (not learnable)\n",
        "        self.register_buffer('running_mean', torch.zeros(num_features))\n",
        "        self.register_buffer('running_var', torch.ones(num_features))\n",
        "        self.register_buffer('num_batches_tracked', torch.tensor(0, dtype=torch.long))\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Input shape: (N, C, H, W)\n",
        "        # N = batch size, C = channels, H = height, W = width\n",
        "\n",
        "        if self.training:\n",
        "            # Calculate batch statistics across (N, H, W), keep C dimension\n",
        "            mean = x.mean(dim=[0, 2, 3], keepdim=False)  # Shape: (C,)\n",
        "            var = x.var(dim=[0, 2, 3], keepdim=False, unbiased=False)  # Shape: (C,)\n",
        "\n",
        "            # CRITICAL: Detach mean and var from computation graph\n",
        "            # This prevents gradients from flowing through these statistics\n",
        "            mean_detached = mean.detach()\n",
        "            var_detached = var.detach()\n",
        "\n",
        "            # Update running statistics (no gradients needed here)\n",
        "            with torch.no_grad():\n",
        "                self.running_mean = (1 - self.momentum) * self.running_mean + \\\n",
        "                                   self.momentum * mean_detached\n",
        "                self.running_var = (1 - self.momentum) * self.running_var + \\\n",
        "                                  self.momentum * var_detached\n",
        "                self.num_batches_tracked += 1\n",
        "\n",
        "            # Normalize using DETACHED statistics\n",
        "            # Reshape for broadcasting: (1, C, 1, 1)\n",
        "            x_normalized = (x - mean_detached.view(1, -1, 1, 1)) / \\\n",
        "                          torch.sqrt(var_detached.view(1, -1, 1, 1) + self.eps)\n",
        "\n",
        "        else:\n",
        "            # Inference mode: use running statistics\n",
        "            x_normalized = (x - self.running_mean.view(1, -1, 1, 1)) / \\\n",
        "                          torch.sqrt(self.running_var.view(1, -1, 1, 1) + self.eps)\n",
        "\n",
        "        # Scale and shift using learnable parameters\n",
        "        # weight (gamma) and bias (beta) WILL have gradients\n",
        "        out = self.weight.view(1, -1, 1, 1) * x_normalized + \\\n",
        "              self.bias.view(1, -1, 1, 1)\n",
        "\n",
        "        return out\n",
        "\n",
        "    def extra_repr(self):\n",
        "        return f'{self.num_features}, eps={self.eps}, momentum={self.momentum}'"
      ],
      "metadata": {
        "id": "NxdH67V4YP9-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vlCAjBEWwXNo"
      },
      "source": [
        "# defining resnet models\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(\n",
        "            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = CustomBatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
        "                               stride=1, padding=1, bias=False)\n",
        "        self.bn2 = CustomBatchNorm2d(planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                CustomBatchNorm2d(self.expansion*planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
        "                               stride=stride, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.conv3 = nn.Conv2d(planes, self.expansion *\n",
        "                               planes, kernel_size=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion*planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = F.relu(self.bn2(self.conv2(out)))\n",
        "        out = self.bn3(self.conv3(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, num_blocks, num_classes=10):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_planes = 64\n",
        "\n",
        "        # This is the \"stem\"\n",
        "        # For CIFAR (32x32 images), it does not perform downsampling\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3,\n",
        "                               stride=1, padding=1, bias=False)\n",
        "        self.bn1 = CustomBatchNorm2d(64)\n",
        "        # four stages with three downsampling\n",
        "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
        "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1]*(num_blocks-1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, stride))\n",
        "            self.in_planes = planes * block.expansion\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = F.avg_pool2d(out, 4)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "def ResNet18():\n",
        "    return ResNet(BasicBlock, [2, 2, 2, 2])\n",
        "\n",
        "\n",
        "def ResNet34():\n",
        "    return ResNet(BasicBlock, [3, 4, 6, 3])\n",
        "\n",
        "\n",
        "def ResNet50():\n",
        "    return ResNet(Bottleneck, [3, 4, 6, 3])\n",
        "\n",
        "\n",
        "def ResNet101():\n",
        "    return ResNet(Bottleneck, [3, 4, 23, 3])\n",
        "\n",
        "\n",
        "def ResNet152():\n",
        "    return ResNet(Bottleneck, [3, 8, 36, 3])\n",
        "\n",
        "\n",
        "def test_resnet18():\n",
        "    net = ResNet18()\n",
        "    y = net(torch.randn(1, 3, 32, 32))\n",
        "    print(y.size())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "CrfVayc0LCC5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Batch Normalization\n",
        "\n",
        "Chosen LR from prev experiment = 0.01\n",
        "\n",
        "Chosen Weight decay =  1e-2\n",
        "\n",
        "Epochs = 300\n",
        "\n",
        "With Cosine Annealing\n",
        "\n",
        "Other hyperparameters unchanged"
      ],
      "metadata": {
        "id": "pcOueHiXenry"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chosen_LR = 0.01\n",
        "chosen_WD =  1e-2\n",
        "\n",
        "# main body\n",
        "config = {\n",
        "    'lr': chosen_LR,\n",
        "    'momentum': 0.9,\n",
        "    'weight_decay': chosen_WD\n",
        "}"
      ],
      "metadata": {
        "id": "DKbsbBxPf-tO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Section 6: Custom Batch Normalization\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"Section 6: Custom Batch Normalization\")\n",
        "print(f\"{'='*60}\\n\")\n",
        "\n",
        "# Create model with CustomBatchNorm2d\n",
        "net = ResNet18().to('cuda')\n",
        "criterion = nn.CrossEntropyLoss().to('cuda')\n",
        "optimizer = optim.SGD(net.parameters(), lr=config['lr'],\n",
        "                      momentum=config['momentum'],weight_decay=config['weight_decay'])\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=300)\n",
        "\n",
        "# Lists to store losses and accuracies\n",
        "train_losses = []\n",
        "train_accs = []\n",
        "val_losses = []\n",
        "val_accs = []\n",
        "\n",
        "for epoch in range(1, 301):\n",
        "    train_loss, train_acc = train(epoch, net, criterion, trainloader, scheduler)\n",
        "    val_loss, val_acc = test(epoch, net, criterion, valloader)\n",
        "\n",
        "    # Store values\n",
        "    train_losses.append(train_loss)\n",
        "    train_accs.append(train_acc)\n",
        "    val_losses.append(val_loss)\n",
        "    val_accs.append(val_acc)\n",
        "\n",
        "    print((\"Epoch : %3d, training loss : %0.4f, training accuracy : %2.2f, val loss \" + \\\n",
        "          \": %0.4f, val accuracy : %2.2f\") % (epoch, train_loss, train_acc, val_loss, val_acc))\n",
        "\n",
        "# Plot\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "ax1.plot(range(1, 301), train_losses, 'b-', label='Train', linewidth=2)\n",
        "ax1.plot(range(1, 301), val_losses, 'r-', label='Validation', linewidth=2)\n",
        "ax1.set_xlabel(\"Epoch\", fontsize=12)\n",
        "ax1.set_ylabel(\"Loss\", fontsize=12)\n",
        "ax1.set_title(\"Loss vs Epochs (Custom BatchNorm)\", fontsize=14)\n",
        "ax1.legend(fontsize=11)\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "ax2.plot(range(1, 301), train_accs, 'b-', label='Train', linewidth=2)\n",
        "ax2.plot(range(1, 301), val_accs, 'r-', label='Validation', linewidth=2)\n",
        "ax2.set_xlabel(\"Epoch\", fontsize=12)\n",
        "ax2.set_ylabel(\"Accuracy (%)\", fontsize=12)\n",
        "ax2.set_title(\"Accuracy vs Epochs (Custom BatchNorm)\", fontsize=14)\n",
        "ax2.legend(fontsize=11)\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('section6_custom_batchnorm.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# Save the model\n",
        "torch.save(net.state_dict(), 'section6_best_model.pth')\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"Final Results - Custom BatchNorm:\")\n",
        "print(f\"  Train Loss: {train_losses[-1]:.4f}, Train Acc: {train_accs[-1]:.2f}%\")\n",
        "print(f\"  Val Loss: {val_losses[-1]:.4f}, Val Acc: {val_accs[-1]:.2f}%\")\n",
        "print(f\"{'='*60}\\n\")"
      ],
      "metadata": {
        "id": "IOfLyDDsXxhR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}